{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************************\n",
    "#**************************************************************\n",
    "#**************************************************************\n",
    "# \n",
    "# - This Python file contains the functions required for \n",
    "# demonstrating the functionality of Bayesian maximum variance\n",
    "# inclusion (BMVI) sampling method. Implementations for BMVI, LPM and\n",
    "# SRS are given. The prediction method used with all samplers\n",
    "# is the standard regularized least squares (RLS) method with \n",
    "# a linear kernel. Prediction performance and hyperparameter \n",
    "# selection is implemented via 10-fold cross-validation. \n",
    "# \n",
    "# If one wishes to implement BMVI via other prediction methods\n",
    "# then the respective Hessian and gradient (see the corresponding\n",
    "# article) needs to be edited. \n",
    "#\n",
    "#**************************************************************\n",
    "#**************************************************************\n",
    "#**************************************************************\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function implements the Bayesian maximum\n",
    "# variance inclusion (BMVI) sampling for a given data set. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "# 'n_samples' integer, the number of data points to be sampled. \n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'inclusion_index_set' a list of data indexes sampled.\n",
    "# 'generalization_error_list' a list of estimated predicton \n",
    "# errors as a function of number of sampled data points. \n",
    "#\n",
    "###############################################################\n",
    "def BMVI(X, y, n_samples):\n",
    "    unsampled_data_set_indexes = np.array(range(0, len(y)))\n",
    "    inclusion_index_set = []\n",
    "    generalization_error_list = []\n",
    "    posterior_variance_list = None\n",
    "    if n_samples >= len(y):\n",
    "        n_samples = len(y)-1\n",
    "    for sample_ind in range(0, n_samples):\n",
    "        if np.mod(sample_ind, 100) == 0:\n",
    "            print('BMVI sampling ' + str(sample_ind+1) + 'th data point (' + str(n_samples+1) + ' in total)')\n",
    "        if sample_ind == 0: # First sample, random start\n",
    "            start_ind = np.random.randint(0, len(y), 2) # We need to have two initial samples because of cross-validation\n",
    "            inclusion_index_set.append(unsampled_data_set_indexes[start_ind][:])\n",
    "            inclusion_index_set = inclusion_index_set[0]\n",
    "            inclusion_index_set = inclusion_index_set.tolist()\n",
    "            unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, start_ind)\n",
    "        else: # \n",
    "            # Do BMVI selection\n",
    "            if len(posterior_variance_list) > 0:\n",
    "                max_var_ind = np.where(posterior_variance_list == np.max(posterior_variance_list))[0][0]\n",
    "                inclusion_index_set.append(unsampled_data_set_indexes[max_var_ind])\n",
    "                unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, max_var_ind) \n",
    "        X_sampled = X[inclusion_index_set, :]\n",
    "        y_sampled = y[inclusion_index_set]\n",
    "        X_unsampled = X[unsampled_data_set_indexes, :]\n",
    "        y_unsampled = y[unsampled_data_set_indexes]\n",
    "        # Train a RLS prediction model on the currently sampled data \n",
    "        w_mp, Hessian = solveRLS(X_sampled, y_sampled)\n",
    "        y_predict = X_unsampled@w_mp\n",
    "        posterior_variance_list = []\n",
    "        for i in range(0, X_unsampled.shape[0]):\n",
    "            posterior_variance_list.append(X_unsampled[i,:]@np.linalg.pinv(Hessian)@np.transpose(X_unsampled[i,:]))\n",
    "        generalization_error_list.append(np.mean(np.abs(y_predict-y_unsampled)))\n",
    "    print(\"\\n\")\n",
    "    return inclusion_index_set, generalization_error_list\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function implements the local pivotal\n",
    "# method (LPM) for a given data set. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "# 'n_samples' integer, the number of data points to be sampled. \n",
    "# 'p' initial inclusion probability. This determines how many data \n",
    "# points LPM chooses to incluse. \n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'inclusion_index_set' a list of data indexes sampled.\n",
    "# 'generalization_error_list' a list of estimated predicton \n",
    "# errors as a function of number of sampled data points. \n",
    "#\n",
    "###############################################################\n",
    "def LPM(X, y, n_samples, p):\n",
    "    unsampled_data_set_indexes = np.array(range(0, len(y)))\n",
    "    inclusion_index_set = []\n",
    "    generalization_error_list = []\n",
    "    # index_probs contains information about selection order and inclusion probabilities. \n",
    "    index_probs = np.array([range(0, len(y)), np.ones((len(y)))*p, np.zeros((len(y)))])\n",
    "    # Temp is used for updating the above variable.  \n",
    "    temp = index_probs[:, (np.where(index_probs[1,:] > 0) and np.where(index_probs[1,:] < 1))[0]]\n",
    "    cnt = 0\n",
    "    # With LPM we first determine the sampling design and the do prediction estimation. \n",
    "    while temp.shape[1] > 1 and np.sum(temp[1,:]) >= 1: # Latter condition is required to ensure the loop does not continue indefinitely. \n",
    "        rand_ind = np.random.randint(0, temp.shape[1])\n",
    "        sample_ind = int(temp[0, rand_ind])\n",
    "        temp = np.delete(temp, rand_ind, axis=1)\n",
    "        p1 = index_probs[1, sample_ind]\n",
    "        sample_X = X[sample_ind, :]\n",
    "        neighbors_X = X[temp[0,:].astype(int), :]\n",
    "        dist_mat = np.sqrt(np.sum(np.power(neighbors_X-sample_X, 2), axis=1))\n",
    "        min_dist_index = np.where(dist_mat == np.min(dist_mat))[0][0]\n",
    "        neighbor_ind = int(temp[0, min_dist_index])\n",
    "        p2 = temp[1, min_dist_index]\n",
    "        randNum = np.random.uniform()\n",
    "        if p1+p2 < 1:\n",
    "            if randNum < p1/float((p1+p2)):\n",
    "                p1 = p1+p2\n",
    "                p2 = 0\n",
    "            else:\n",
    "                p1 = 0\n",
    "                p2 = p1+p2\n",
    "        elif p1+p2 >= 1:\n",
    "            if randNum < (1-p1)/float((2-p1-p2)):\n",
    "                p1 = p1+p2-1\n",
    "                p2 = 1\n",
    "            else:\n",
    "                p1 = 1\n",
    "                p2 = p1+p2-1\n",
    "        index_probs[1, sample_ind] = p1\n",
    "        index_probs[1, neighbor_ind] = p2\n",
    "        if p1 == 1:\n",
    "            index_probs[2, sample_ind] = cnt\n",
    "            cnt += 1\n",
    "        elif p2 == 1:\n",
    "            index_probs[2, neighbor_ind] = cnt\n",
    "            cnt += 1\n",
    "        temp = index_probs[:, (np.where(index_probs[1,:] > 0) and (np.where(index_probs[1,:] < 1)))[0]]\n",
    "    # Sample the last indexes last\n",
    "    unsampled_inds = np.where(index_probs[1,:] < 1)[0]\n",
    "    cnt = np.max(index_probs[2,:]) + 1\n",
    "    if unsampled_inds.size > 0:\n",
    "        cnt = np.max(index_probs[2,:]) + 1\n",
    "        for ind in unsampled_inds:\n",
    "            index_probs[1,ind] = 1\n",
    "            index_probs[2,ind] = cnt\n",
    "            cnt += 1\n",
    "    # Sort the data indexes based on selection order. \n",
    "    index_probs = index_probs[:, np.argsort(index_probs[2,:])]\n",
    "    inclusion_index_set = index_probs[0,:]\n",
    "    # Implement cross-validation and prediction. \n",
    "    for i in range(2, len(inclusion_index_set)-1): # We can't sample all data and do prediction estimation. Minumum 2 data points required. \n",
    "        if np.mod(i, 100) == 0:\n",
    "            print('LPM sampling ' + str(i+1) + 'th data point (' + str(len(inclusion_index_set)) + ' in total)')\n",
    "        X_sampled = X[inclusion_index_set[0:i].astype(int), :]\n",
    "        y_sampled = y[inclusion_index_set[0:i].astype(int)]\n",
    "        X_unsampled = X[inclusion_index_set[i:len(inclusion_index_set)].astype(int), :]\n",
    "        y_unsampled = y[inclusion_index_set[i:len(inclusion_index_set)].astype(int)]\n",
    "        # Train a RLS prediction model on the currently sampled data \n",
    "        w_mp, Hessian = solveRLS(X_sampled, y_sampled)\n",
    "        y_predict = X_unsampled@w_mp\n",
    "        generalization_error_list.append(np.mean(np.abs(y_predict-y_unsampled)))\n",
    "    print(\"\\n\")\n",
    "    return inclusion_index_set, generalization_error_list\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function implements the simple random \n",
    "# sampling (SRS) method for a given data set. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "# 'n_samples' integer, the number of data points to be sampled. \n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'inclusion_index_set' a list of data indexes sampled.\n",
    "# 'generalization_error_list' a list of estimated predicton \n",
    "# errors as a function of number of sampled data points. \n",
    "#\n",
    "###############################################################\n",
    "def SRS(X, y, n_samples):\n",
    "    unsampled_data_set_indexes = np.array(range(0, len(y)))\n",
    "    inclusion_index_set = []\n",
    "    generalization_error_list = []\n",
    "    if n_samples >= len(y):\n",
    "        n_samples = len(y)-1\n",
    "    for sample_ind in range(0, n_samples):\n",
    "        if np.mod(sample_ind, 100) == 0:\n",
    "            print('SRS sampling ' + str(sample_ind+1) + 'th data point (' + str(n_samples+1) + ' in total)')\n",
    "        if sample_ind == 0: # First sample, random start\n",
    "            start_ind = np.random.randint(0, len(y), 2) # We need to have two initial samples because of cross-validation\n",
    "            inclusion_index_set.append(unsampled_data_set_indexes[start_ind][:])\n",
    "            inclusion_index_set = inclusion_index_set[0]\n",
    "            inclusion_index_set = inclusion_index_set.tolist()\n",
    "            unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, start_ind)\n",
    "        else:\n",
    "            rand_ind = np.random.randint(0, len(unsampled_data_set_indexes))\n",
    "            inclusion_index_set.append(unsampled_data_set_indexes[rand_ind])\n",
    "            unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, rand_ind)\n",
    "        X_sampled = X[inclusion_index_set, :]\n",
    "        y_sampled = y[inclusion_index_set]\n",
    "        X_unsampled = X[unsampled_data_set_indexes, :]\n",
    "        y_unsampled = y[unsampled_data_set_indexes]\n",
    "        # Train a RLS prediction model on the currently sampled data \n",
    "        w_mp, Hessian = solveRLS(X_sampled, y_sampled)\n",
    "        y_predict = X_unsampled@w_mp\n",
    "        generalization_error_list.append(np.mean(np.abs(y_predict-y_unsampled)))\n",
    "    print(\"\\n\")\n",
    "    return inclusion_index_set, generalization_error_list\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function produces a cross-validation \n",
    "# fold partitioning. \n",
    "# \n",
    "# - INPUTS: \n",
    "# 'n_samples' integer, the number of data points. \n",
    "# 'n_folds' integer, the number of folds.\n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'folds' a list of integer lists containing fold indices. \n",
    "#\n",
    "###############################################################\n",
    "def makeFolds(n_samples, n_folds):\n",
    "    folds = []\n",
    "    index_list = np.random.permutation(n_samples)\n",
    "    # Check that the number of data points is larger than required number of folds\n",
    "    if n_samples > n_folds:\n",
    "        fold_size = np.floor(n_samples/float(n_folds))\n",
    "        for fold in range(0, n_folds):\n",
    "            start_ind = int(fold_size*fold)\n",
    "            end_ind = int(fold_size*(fold+1))\n",
    "            if fold < n_folds-1:\n",
    "                folds.append(index_list[start_ind:end_ind].tolist())\n",
    "            else:\n",
    "                folds.append(index_list[start_ind:].tolist())\n",
    "        return folds\n",
    "    # Otherwise, we create a leave-one-out fold partitioning. \n",
    "    else:\n",
    "        for i in range(0, len(index_list)):\n",
    "            folds.append([index_list[i]])\n",
    "        return folds\n",
    "    \n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function solves the maximum likelihood (ML)\n",
    "# regularized least squares model. Hyperparameter selection \n",
    "# is conducted using 10-fold cross-validation. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'optimal_w_mp' the hyperparameter tuned ML weight vector for \n",
    "# RLS model.\n",
    "# 'Hessian' the optimal Hessian matrix corresponding to \n",
    "# matrix A of function S(w) in equation (8) in the article. \n",
    "#\n",
    "###############################################################\n",
    "def solveRLS(X, y):\n",
    "    # Choose alpha/beta hyperparameters from an exponential\n",
    "    # grid.\n",
    "    alpha_grid = float(2)**np.arange(-7,7)\n",
    "    beta_grid = float(2)**np.arange(-7,7)\n",
    "    # Optimal hyperparameters and auxiliary variables\n",
    "    optimal_alpha = None\n",
    "    optimal_beta = None\n",
    "    optimal_error = np.inf\n",
    "    alphabeta_matrix = np.zeros((X.shape[1], X.shape[1]))\n",
    "    alphabeta_matrix[1:,1:] = np.eye(X.shape[1]-1)\n",
    "    # Loop through the hyperparameter grid \n",
    "    folds = makeFolds(X.shape[0], 10)\n",
    "    for alpha in alpha_grid:\n",
    "        for beta in beta_grid: \n",
    "            prediction_error_list = []\n",
    "            # Choose best hyperparameters via cross-validation\n",
    "            for fold_ind in range(0, len(folds)):\n",
    "                ind = folds[fold_ind]\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "                X_train = np.delete(X_train, ind, 0)\n",
    "                y_train = np.delete(y_train, ind, 0)\n",
    "                X_test = X[ind,:]\n",
    "                y_test = y[ind]\n",
    "                X_train_T = np.transpose(X_train)\n",
    "                # Solve the maximum likelihood model\n",
    "                w_mp = np.linalg.inv(X_train_T@X_train + alpha/float(beta)*alphabeta_matrix)@X_train_T@y_train\n",
    "                # Make prediction to validation data\n",
    "                y_pred = X_test@w_mp\n",
    "                prediction_error_list.append(np.sum(np.abs(y_pred-y_test)))\n",
    "            # Evaluate the prediction error on evaluation data and save the best found parameters\n",
    "            if np.sum(prediction_error_list) < optimal_error:\n",
    "                optimal_error = np.sum(prediction_error_list)\n",
    "                optimal_alpha = alpha\n",
    "                optimal_beta = beta\n",
    "    # Get the optimal parameters and return to caller\n",
    "    X_T = np.transpose(X)\n",
    "    optimal_w_mp = np.linalg.inv(X_T@X + optimal_alpha/float(optimal_beta)*alphabeta_matrix)@X_T@y\n",
    "    Hessian = optimal_beta*X_T@X + optimal_alpha*alphabeta_matrix\n",
    "    return optimal_w_mp, Hessian"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
