{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************************\n",
    "#**************************************************************\n",
    "#**************************************************************\n",
    "# \n",
    "# - This Python file contains the functions required for \n",
    "# demonstrating the functionality of Bayesian maximum variance\n",
    "# inclusion sampling method. Implementations for both BMVI and\n",
    "# SRS are given. The prediction methods used with both samplers\n",
    "# is the standard regularized least squares (RLS) method with \n",
    "# a linear kernel. Prediction performance and hyperparameter \n",
    "# selection is implemented via 10-fold cross-validation. \n",
    "# \n",
    "# If one wishes to implement BMVI via other prediction methods\n",
    "# then the respective Hessian and gradient (see the corresponding\n",
    "# article) needs to be edited. \n",
    "#\n",
    "#**************************************************************\n",
    "#**************************************************************\n",
    "#**************************************************************\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function implements the Bayesian maximum\n",
    "# variance inclusion (BMVI) sampling for a given data set. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "# 'n_samples' integer, the number of data points to be sampled. \n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'inclusion_index_set' a list of data indexes sampled.\n",
    "# 'generalization_error_list' a list of estimated predicton \n",
    "# errors as a function of number of sampled data points. \n",
    "#\n",
    "###############################################################\n",
    "def BMVI(X, y, n_samples):\n",
    "    unsampled_data_set_indexes = np.array(range(0, len(y)))\n",
    "    inclusion_index_set = []\n",
    "    generalization_error_list = []\n",
    "    posterior_variance_list = None\n",
    "    if n_samples >= len(y):\n",
    "        n_samples = len(y)-1\n",
    "    for sample_ind in range(0, n_samples):\n",
    "        if np.mod(sample_ind, 100) == 0:\n",
    "            print('BMVI sampling ' + str(sample_ind+1) + 'th data point (' + str(n_samples+1) + ' in total)')\n",
    "        if sample_ind == 0: # First sample, random start\n",
    "            start_ind = np.random.randint(0, len(y), 2) # We need to have two initial samples because of cross-validation\n",
    "            inclusion_index_set.append(unsampled_data_set_indexes[start_ind][:])\n",
    "            inclusion_index_set = inclusion_index_set[0]\n",
    "            inclusion_index_set = inclusion_index_set.tolist()\n",
    "            unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, start_ind)\n",
    "        else: # \n",
    "            # Do BMVI selection\n",
    "            if len(posterior_variance_list) > 0:\n",
    "                max_var_ind = np.where(posterior_variance_list == np.max(posterior_variance_list))[0][0]\n",
    "                inclusion_index_set.append(unsampled_data_set_indexes[max_var_ind])\n",
    "                unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, max_var_ind) \n",
    "        X_sampled = X[inclusion_index_set, :]\n",
    "        y_sampled = y[inclusion_index_set]\n",
    "        X_unsampled = X[unsampled_data_set_indexes, :]\n",
    "        y_unsampled = y[unsampled_data_set_indexes]\n",
    "        # Train a RLS prediction model on the currently sampled data \n",
    "        w_mp, Hessian = solveRLS(X_sampled, y_sampled)\n",
    "        y_predict = X_unsampled@w_mp\n",
    "        posterior_variance_list = []\n",
    "        for i in range(0, X_unsampled.shape[0]):\n",
    "            posterior_variance_list.append(X_unsampled[i,:]@np.linalg.pinv(Hessian)@np.transpose(X_unsampled[i,:]))\n",
    "        generalization_error_list.append(np.mean(np.abs(y_predict-y_unsampled)))\n",
    "    print(\"\\n\")\n",
    "    return inclusion_index_set, generalization_error_list\n",
    "\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function implements the simple random \n",
    "# sampling (SRS) method for a given data set. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "# 'n_samples' integer, the number of data points to be sampled. \n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'inclusion_index_set' a list of data indexes sampled.\n",
    "# 'generalization_error_list' a list of estimated predicton \n",
    "# errors as a function of number of sampled data points. \n",
    "#\n",
    "###############################################################\n",
    "def SRS(X, y, n_samples):\n",
    "    unsampled_data_set_indexes = np.array(range(0, len(y)))\n",
    "    inclusion_index_set = []\n",
    "    generalization_error_list = []\n",
    "    posterior_variance_list = None\n",
    "    if n_samples >= len(y):\n",
    "        n_samples = len(y)-1\n",
    "    for sample_ind in range(0, n_samples):\n",
    "        if np.mod(sample_ind, 100) == 0:\n",
    "            print('SRS sampling ' + str(sample_ind+1) + 'th data point (' + str(n_samples+1) + ' in total)')\n",
    "        if sample_ind == 0: # First sample, random start\n",
    "            start_ind = np.random.randint(0, len(y), 2) # We need to have two initial samples because of cross-validation\n",
    "            inclusion_index_set.append(unsampled_data_set_indexes[start_ind][:])\n",
    "            inclusion_index_set = inclusion_index_set[0]\n",
    "            inclusion_index_set = inclusion_index_set.tolist()\n",
    "            unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, start_ind)\n",
    "        else:\n",
    "            rand_ind = np.random.randint(0, len(unsampled_data_set_indexes))\n",
    "            inclusion_index_set.append(unsampled_data_set_indexes[rand_ind])\n",
    "            unsampled_data_set_indexes = np.delete(unsampled_data_set_indexes, rand_ind)\n",
    "            X_sampled = X[inclusion_index_set, :]\n",
    "            y_sampled = y[inclusion_index_set]\n",
    "            X_unsampled = X[unsampled_data_set_indexes, :]\n",
    "            y_unsampled = y[unsampled_data_set_indexes]\n",
    "            # Train a RLS prediction model on the currently sampled data \n",
    "            w_mp, Hessian = solveRLS(X_sampled, y_sampled)\n",
    "            y_predict = X_unsampled@w_mp\n",
    "            generalization_error_list.append(np.mean(np.abs(y_predict-y_unsampled)))\n",
    "    print(\"\\n\")\n",
    "    return inclusion_index_set, generalization_error_list\n",
    "\n",
    "\n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function produces a cross-validation \n",
    "# fold partitioning. \n",
    "# \n",
    "# - INPUTS: \n",
    "# 'n_samples' integer, the number of data points. \n",
    "# 'n_folds' integer, the number of folds.\n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'folds' a list of integer lists containing fold indices. \n",
    "#\n",
    "###############################################################\n",
    "def makeFolds(n_samples, n_folds):\n",
    "    folds = []\n",
    "    index_list = np.random.permutation(n_samples)\n",
    "    # Check that the number of data points is larger than required number of folds\n",
    "    if n_samples > n_folds:\n",
    "        fold_size = np.floor(n_samples/float(n_folds))\n",
    "        for fold in range(0, n_folds):\n",
    "            start_ind = int(fold_size*fold)\n",
    "            end_ind = int(fold_size*(fold+1))\n",
    "            if fold < n_folds-1:\n",
    "                folds.append(index_list[start_ind:end_ind].tolist())\n",
    "            else:\n",
    "                folds.append(index_list[start_ind:].tolist())\n",
    "        return folds\n",
    "    # Otherwise, we create a leave-one-out fold partitioning. \n",
    "    else:\n",
    "        for i in range(0, len(index_list)):\n",
    "            folds.append([index_list[i]])\n",
    "        return folds\n",
    "\n",
    "    \n",
    "###############################################################\n",
    "#\n",
    "# - DESCRIPTION: This function solves the maximum likelihood (ML)\n",
    "# regularized least squares model. Hyperparameter selection \n",
    "# is conducted using 10-fold cross-validation. \n",
    "# \n",
    "# - INPUTS: \n",
    " # 'X' X contains the input data with first column assumed to \n",
    "# be all ones (constant term). Each row corresponds to one\n",
    "# observation.\n",
    "# 'y' corresponds to the vector of output values.\n",
    "#\n",
    "# - OUTPUTS: \n",
    "# 'optimal_w_mp' the hyperparameter tuned ML weight vector for \n",
    "# RLS model.\n",
    "# 'Hessian' the optimal Hessian matrix corresponding to \n",
    "# matrix A of function S(w) in equation (8) in the article. \n",
    "#\n",
    "###############################################################\n",
    "def solveRLS(X, y):\n",
    "    # Choose alpha/beta hyperparameters from an exponential\n",
    "    # grid.\n",
    "    alpha_grid = float(2)**np.arange(-7,7)\n",
    "    beta_grid = float(2)**np.arange(-7,7)\n",
    "    # Optimal hyperparameters and auxiliary variables\n",
    "    optimal_alpha = None\n",
    "    optimal_beta = None\n",
    "    optimal_error = np.inf\n",
    "    alphabeta_matrix = np.zeros((X.shape[1], X.shape[1]))\n",
    "    alphabeta_matrix[1:,1:] = np.eye(X.shape[1]-1)\n",
    "    # Loop through the hyperparameter grid \n",
    "    folds = makeFolds(X.shape[0], 10)\n",
    "    for alpha in alpha_grid:\n",
    "        for beta in beta_grid: \n",
    "            prediction_error_list = []\n",
    "            # Choose best hyperparameters via cross-validation\n",
    "            for fold_ind in range(0, len(folds)):\n",
    "                ind = folds[fold_ind]\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "                X_train = np.delete(X_train, ind, 0)\n",
    "                y_train = np.delete(y_train, ind, 0)\n",
    "                X_test = X[ind,:]\n",
    "                y_test = y[ind]\n",
    "                X_train_T = np.transpose(X_train)\n",
    "                # Solve the maximum likelihood model\n",
    "                w_mp = np.linalg.inv(X_train_T@X_train + alpha/float(beta)*alphabeta_matrix)@X_train_T@y_train\n",
    "                # Make prediction to validation data\n",
    "                y_pred = X_test@w_mp\n",
    "                prediction_error_list.append(np.sum(np.abs(y_pred-y_test)))\n",
    "            # Evaluate the prediction error on evaluation data and save the best found parameters\n",
    "            if np.sum(prediction_error_list) < optimal_error:\n",
    "                optimal_error = np.sum(prediction_error_list)\n",
    "                optimal_alpha = alpha\n",
    "                optimal_beta = beta\n",
    "    # Get the optimal parameters and return to caller\n",
    "    X_T = np.transpose(X)\n",
    "    optimal_w_mp = np.linalg.inv(X_T@X + optimal_alpha/float(optimal_beta)*alphabeta_matrix)@X_T@y\n",
    "    Hessian = optimal_beta*X_T@X + optimal_alpha*alphabeta_matrix\n",
    "    return optimal_w_mp, Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
